---
---
@article{kitson2021survey,
  abbr={AIR},
  bibtex_show = {true},
  url = {https://link.springer.com/article/10.1007/s10462-022-10351-w},
  title={A survey of Bayesian Network structure learning},
  abstract={Bayesian Networks (BNs) have become increasingly popular over the last few decades as a tool for reasoning under uncertainty in fields as diverse as medicine, biology, epidemiology, economics and the social sciences. This is especially true in real-world areas where we seek to answer complex questions based on hypothetical evidence to determine actions for intervention. However, determining the graphical structure of a BN remains a major challenge, especially when modelling a problem under causal assumptions. Solutions to this problem include the automated discovery of BN graphs from data, constructing them based on expert knowledge, or a combination of the two. This paper provides a comprehensive review of combinatoric algorithms proposed for learning BN structure from data, describing 74 algorithms including prototypical, well-established and state-of-the-art approaches. The basic approach of each algorithm is described in consistent terms, and the similarities and differences between them highlighted. Methods of evaluating algorithms and their comparative performance are discussed including the consistency of claims made in the literature. Approaches for dealing with data noise in real-world datasets and incorporating expert knowledge into the learning process are also covered.},
  author={Kitson, Neville K and Constantinou, Anthony C and Guo, Zhigao and Liu, Yang and Chobtham, Kiattikun},
  journal={Artificial Intelligence Review},
  page={1-94},
  year={2023},
  doi={10.1007/s10462-022-10351-w},
  altmetric={141423578},
  pdf={A survey of Bayesian Network structure learning.pdf},
  dimensions={true},
  google_scholar_id={0EnyYjriUFMC}
}

@article{liu2022greedy,
  abbr={MLJ},
  abstract={Learning from data that contain missing values represents a common phenomenon in many domains. Relatively few Bayesian Network structure learning algorithms account for missing data, and those that do tend to rely on standard approaches that assume missing data are missing at random, such as the Expectation-Maximisation algorithm. Because missing data are often systematic, there is a need for more pragmatic methods that can effectively deal with data sets containing missing values not missing at random. The absence of approaches that deal with systematic missing data impedes the application of BN structure learning methods to real-world problems where missingness are not random. This paper describes three variants of greedy search structure learning that utilise pairwise deletion and inverse probability weighting to maximally leverage the observed data and to limit potential bias caused by missing values. The first two of the variants can be viewed as sub-versions of the third and best performing variant, but are important in their own in illustrating the successive improvements in learning accuracy. The empirical investigations show that the proposed approach outperforms the commonly used and state-of-the-art Structural EM algorithm, both in terms of learning accuracy and efficiency, as well as both when data are missing at random and not at random.},
  bibtex_show = {true},
  url = {https://link.springer.com/article/10.1007/s10994-022-06195-8},
  Code = {https://github.com/Enderlogic/HC-missing-data},
  title={Greedy structure learning from data that contain systematic missing values},
  author={Liu, Yang and Constantinou, Anthony C},
  journal={Machine Learning},
  volume={111},
  number={10},
  pages={3867--3896},
  year={2022},
  publisher={Springer US},
  Selected = {true},
  doi={10.1007/s10994-022-06195-8},
  altmetric={133919364},
  pdf={Greedy structure learning from data that contain systematic missing values.pdf},
  dimensions={true},
  google_scholar_id={5nxA0vEk-isC}
}

@article{constantinou2021large,
  abbr={IJAR},
  abstract={Numerous Bayesian Network (BN) structure learning algorithms have been proposed in the literature over the past few decades. Each publication makes an empirical or theoretical case for the algorithm proposed in that publication and results across studies are often inconsistent in their claims about which algorithm is ‘best’. This is partly because there is no agreed evaluation approach to determine their effectiveness. Moreover, each algorithm is based on a set of assumptions, such as complete data and causal sufficiency, and tend to be evaluated with data that conforms to these assumptions, however unrealistic these assumptions may be in the real world. As a result, it is widely accepted that synthetic performance overestimates real performance, although to what degree this may happen remains unknown. This paper investigates the performance of 15 state-of-the-art, well-established, or recent promising structure learning algorithms. We propose a methodology that applies the algorithms to data that incorporates synthetic noise, in an effort to better understand the performance of structure learning algorithms when applied to real data. Each algorithm is tested over multiple case studies, sample sizes, types of noise, and assessed with multiple evaluation criteria. This work involved learning approximately 10,000 graphs with a total structure learning runtime of seven months. In investigating the impact of data noise, we provide the first large scale empirical comparison of BN structure learning algorithms under different assumptions of data noise. The results suggest that traditional synthetic performance may overestimate real-world performance by anywhere between 10% and more than 50%. They also show that while score-based learning is generally superior to constraint-based learning, a higher fitting score does not necessarily imply a more accurate causal graph. The comparisons extend to other outcomes of interest, such as runtime, reliability, and resilience to noise, assessed over both small and large networks, and with both limited and big data. To facilitate comparisons with future studies, we have made all data, raw results, graphs and BN models freely available online.}
  bibtex_show = {true},
  url = {https://www.sciencedirect.com/science/article/pii/S0888613X21000025},
  title={Large-scale empirical validation of Bayesian Network structure learning algorithms with noisy data},
  author={Constantinou, Anthony C and Liu, Yang and Chobtham, Kiattikun and Guo, Zhigao and Kitson, Neville K},
  journal={International Journal of Approximate Reasoning},
  volume={131},
  pages={151--188},
  year={2021},
  publisher={Elsevier},
  doi={10.1016/j.ijar.2021.01.001}
  pdf={Large-scale empirical validation of Bayesian Network structure learning algorithms with noisy data.pdf},
  dimensions={true},
  google_scholar_id={ZeXyd9-uunAC}
}

@article{constantinou2020bayesys,
  bibtex_show = {true},
  url = {http://constantinou.info/downloads/bayesys/bayesys_repository.pdf},
  title={The Bayesys data and Bayesian network repository},
  author={Constantinou, Anthony C and Liu, Yang and Chobtham, Kiattikun and Guo, Zhigao and Kitson, Neville K},
  journal={Queen Mary University of London: London, UK},
  year={2020},
  pdf={bayesys_repository.pdf},
  dimensions={true},
  google_scholar_id={hC7cP41nSMkC}
}

@article{constantinou2022effective,
  abbr={IJAR},
  abstract={Learning the structure of a Bayesian Network (BN) with score-based solutions involves exploring the search space of possible graphs and moving towards the graph that maximises a given objective function. Some algorithms offer exact solutions that guarantee to return the graph with the highest objective score, while others offer approximate solutions in exchange for reduced computational complexity. This paper describes an approximate BN structure learning algorithm, which we call Model Averaging Hill-Climbing (MAHC), that combines two novel strategies with hill-climbing search. The algorithm starts by pruning the search space of graphs, where the pruning strategy can be viewed as an aggressive version of the pruning strategies that are typically applied to combinatorial optimisation structure learning problems. It then performs model averaging in the hill-climbing search process and moves to the neighbouring graph that maximises the objective function, on average, for that neighbouring graph and over all its valid neighbouring graphs. Comparisons with other algorithms spanning different classes of learning suggest that the combination of aggressive pruning with model averaging is both effective and efficient, particularly in the presence of data noise.},
  bibtex_show = {true},
  url = {https://www.sciencedirect.com/science/article/pii/S0888613X22001591},
  title={Effective and efficient structure learning with pruning and model averaging strategies},
  author={Constantinou, Anthony C and Liu, Yang and Kitson, Neville K and Chobtham, Kiattikun and Guo, Zhigao},
  journal={International Journal of Approximate Reasoning},
  volume={151},
  pages={292--321},
  year={2022},
  publisher={Elsevier},
  pdf={Effective and efficient structure learning with pruning and model averaging strategies.pdf},
  dimension={true},
  google_scholar_id={TQgYirikUcIC}
}